{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an matrix factorization autoencoder while taking the new [torch-sparse](https://github.com/rusty1s/pytorch_sparse) matrix multiplication library out for a spin.\n",
    "\n",
    "This runs an experiment on MovieLens data which works reasonably well, and finds Star Trek movie vectors similar to other Star Trek movies -- promising enough to feel like this technique actually works well.\n",
    "\n",
    "If this technique continues to work it opens up the path to making MF models that are more parallel and more scaleable. It also makes it easier to do variational / active learning stuff, and dovetails with adversarial methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big value of an autoencoder model is that we can predict a client vector in a single pass over just that client's ratings. This is great for real-time use cases and for scaling training: inferring a new client vector with a trained model is trivially parallelizeable. This is not the case at all in our current latent MF models, which need to see all the data to update client vectors.\n",
    "\n",
    "\n",
    "0. **Encoder**. We encode ratings slightly differently than normal. Instead of saying +1 for a positive rating and -1 for a negative rating, we now categorically encode every combination of item and rating type. So if item=1 gets a 0.0 rating we use item_code=0, but if the same item=1 gets a different 1.0 rating we use a different item_code=1. MovieLens has ratings from 0.0 to 5.0 This means the *encoder* learns an embedding for (item_id=1, rating=0.0) and a new embedding for (item_id=1, rating=3.0).\n",
    "\n",
    "\n",
    "1. **User Representation.** This creates a user vector from features (as in autoencoders) rather than memorizing one that recreates features (as in latent / PGM models). This user vector is constructed by matrix multiplying the sparse user-item ratings matrix $R_{ui}$ (of shape n_users x n_items) with a dense item vector matrix $Q_{ik}$ (of shape n_items x k dimensions). \n",
    "\n",
    "    $v = R \\cdot Q / n$ \n",
    "    \n",
    "    In practice we don't materialize the whole sparse $R$ matrix, but take chunks of rows. This follows the typical deep learning paradigm of minibatch training.\n",
    "    \n",
    "    We do the same to compute a user bias from rated items' biases. Here, $B$ is the item bias tensor of shape (n_items, 1)\n",
    "    \n",
    "    $b = R \\cdot B / n$.\n",
    "\n",
    "    The $n$ normalizes the user vectors and biases.  If you look carefully, $R \\cdot Q$ and $R \\cdot B$  are the *sum* of item vectors and biases for all items a single user rated. We want the average, so divide that user's rating count $n$.\n",
    "\n",
    "\n",
    "2. **Sampling**. Mask the encoder ratings. By randomly masking half the ratings in any given iteration we're basically stochastically doing feature bagging and sampling. Because each iteration is random, over many epochs we'll still see all the data -- we're not downsampling. On the decoder, where we measure the likelihood of the data, we're not randomly sampling the data (maybe we should). I haven't tested whether this actually effective -- just a guess.\n",
    "\n",
    "\n",
    "3. **Decoder.** The 'decoder' is in charge of using the user vector $v$ and the user bias $b$ to compute the likelihood. This procedes as a normal latent variable model. We add the user biases $b$ with decoder-specific item biases $c_i$ and interact the user vector $v$ generated in the encoder in the last step with decoder-specific item embedding $t_i$.\n",
    "\n",
    "    $R_{i} \\sim b + v \\cdot t_i + c_i$\n",
    "    \n",
    "Note that here we learn a new decoder-specific item embedding $t_i$. This is completely distinct from the (item_id, rating id) embeddings we defined in the encoder. That means a single item has five encoder vectors (one for each of the five ratings) and has yet another representation in the decoder.\n",
    "\n",
    "A few other differences:\n",
    "\n",
    "- Ratings in previous autoencoders are shuffled. A single player may appear in the first batch and in the last batch of the same epoch. But in this model, we get user \"blocks\" of ratings -- a single user's ratings all fit into the same batch. That's because within each minibatch we want to compute a client vector from all of it's ratings at once.\n",
    "- Validation as defined here isn't really what we want. The validation error is encoding a client's ratings into a compressed latent vector and then measuring the *reconstruction* loss. That's not quite what we want -- we want to give it half of a user's ratings and have it predict the reconstruction of the missing half, caring less about how well it recovers the given first half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fh = np.load('../data/dataset.npz')\n",
    "\n",
    "# We have a bunch of feature columns and last column is the y-target\n",
    "# Note pytorch is finicky about need int64 types\n",
    "train_x = fh['train_x'].astype(np.int64)\n",
    "train_y = fh['train_y']\n",
    "\n",
    "# We've already split into train & test\n",
    "test_x = fh['test_x'].astype(np.int64)\n",
    "test_y = fh['test_y']\n",
    "\n",
    "\n",
    "n_user = int(fh['n_user'])\n",
    "n_item = int(fh['n_item'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll change input item codes to be the outer product of (item_id) x (rating id), effectively getting five times as many item codes.\n",
    "\n",
    "A hacky way to do this is to multiply the item id by 10 (e.g. shifting it over 1 digit), and then add the rating id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_code = train_x[:, 1] * 10 + train_y[:, 0].astype(int)\n",
    "train_x = np.hstack((train_x, item_code[:, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_code = test_x[:, 1] * 10 + test_y[:, 0].astype(int)\n",
    "test_x = np.hstack((test_x, item_code[:, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1  1193    11    10 11935]\n",
      " [    1   914    26    10  9143]\n",
      " [    1  3408     7    10 34084]\n",
      " ...\n",
      " [ 6040   562    37     6  5625]\n",
      " [ 6040  1096   109     6 10964]\n",
      " [ 6040  1097    99     6 10974]]\n",
      " \n",
      "[[5.]\n",
      " [3.]\n",
      " [4.]\n",
      " ...\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "# columns are user_id, item_id and other features \n",
    "# we won't use the 3rd and 4th columns\n",
    "print(train_x)\n",
    "print(' ')\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this function multiplies a sparse matrix with a dense matrix\n",
    "from torch_sparse import spmm\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "class MFAE(nn.Module):\n",
    "    itr = 0\n",
    "    frac = 0.5\n",
    "    \n",
    "    def __init__(self, n_encoder_item, n_decoder_item, k=18, c_vector=1.0, writer=None):\n",
    "        super(MFAE, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.c_vector = c_vector\n",
    "        self.n_item = n_item\n",
    "        self.n_encoder_item = n_encoder_item\n",
    "        self.encoder_bias = Parameter(torch.randn(n_encoder_item, 1) * 1e-6)\n",
    "        self.encoder_vect = Parameter(torch.randn(n_encoder_item, k) * 1e-6)\n",
    "        self.decoder_bias = Parameter(torch.randn(n_decoder_item, 1) * 1e-6)\n",
    "        self.decoder_vect = Parameter(torch.randn(n_decoder_item, k) * 1e-6)\n",
    "    \n",
    "    def __call__(self, indices):\n",
    "        # first column is user index, 2nd is item index and 3rd is\n",
    "        # is an index over (item, rating_type), and not just item.\n",
    "        # In the encoder, we use user index and item-rating-type index\n",
    "        idx = torch.transpose(indices[:, [0, 4]], 1, 0)\n",
    "        n_user_max = indices[:, 0].max() + 1\n",
    "\n",
    "        # The encoder does a\n",
    "        # matrix multiply between a 0 or 1 flag if a feature is present for a user\n",
    "        # and the dense item representation matrix\n",
    "        \n",
    "        values = torch.ones(len(indices))\n",
    "        count = 1 + torch.bincount(indices[:, 0], minlength=n_user_max).float()\n",
    "        # this mask forces us to stochastically use half the\n",
    "        # player's ratings to predict them all\n",
    "        mask = torch.rand(len(indices)) > self.frac\n",
    "        user_bias_sum = spmm(idx[:, mask], values[mask], n_user_max, self.encoder_bias)\n",
    "        user_vect_sum = spmm(idx[:, mask], values[mask], n_user_max, self.encoder_vect)\n",
    "        user_bias_mean = user_bias_sum / count[:, None]\n",
    "        user_vect_mean = user_vect_sum / count[:, None]\n",
    "        # Note user_vector is of size (max(user_idx in batch), k) \n",
    "        # and not (batchsize, k)!\n",
    "    \n",
    "        # Now we're in the decoder\n",
    "        user_idx = indices[:, 0]\n",
    "        item_idx = indices[:, 1]\n",
    "        \n",
    "        # Extract user/item bias/vectors\n",
    "        # Note: we're using a different item representation in the decoder than the encoder\n",
    "        user_bias = user_bias_mean[user_idx]\n",
    "        user_vect = user_vect_mean[user_idx]\n",
    "        item_bias = self.decoder_bias[item_idx]\n",
    "        item_vect = self.decoder_vect[item_idx]\n",
    "\n",
    "        # Compute likelihood\n",
    "        user_item = (item_vect * user_vect).sum(dim=1)[:, None]\n",
    "        log_odds = user_bias + item_bias + user_item\n",
    "        return log_odds\n",
    "\n",
    "    def loss(self, log_odds, target):\n",
    "        loss_mse = F.mse_loss(log_odds, target.float())\n",
    "        \n",
    "        # Compute regularization\n",
    "        prior_ie = l2_regularize(self.encoder_vect) * self.c_vector\n",
    "        prior_id = l2_regularize(self.decoder_vect) * self.c_vector\n",
    "        total = loss_mse + prior_ie + prior_id\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class Loader():\n",
    "    current = 0\n",
    "\n",
    "    def __init__(self, x, y, batchsize=1024, do_shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batchsize = batchsize\n",
    "        self.batches = range(0, len(self.y), batchsize)\n",
    "        self.do_shuffle = do_shuffle\n",
    "        if self.do_shuffle:\n",
    "            # Every epoch re-shuffle the dataset\n",
    "            self.x, self.y = shuffle(self.x, self.y)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Reset & return a new iterator\n",
    "        if self.do_shuffle:\n",
    "            self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches\n",
    "        return int(len(self.x) / self.batchsize)\n",
    "\n",
    "    def __next__(self):\n",
    "        n = self.batchsize\n",
    "        if self.current + n >= len(self.y):\n",
    "            raise StopIteration\n",
    "        i = self.current\n",
    "        xs = torch.from_numpy(self.x[i:i + n])\n",
    "        ys = torch.from_numpy(self.y[i:i + n])\n",
    "        self.current += n\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from ignite.metrics import MeanSquaredError\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/simple_mf_01_2018-11-06_18:58:02.902128\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "# Number of dimensions per user, item\n",
    "k = 10\n",
    "# regularization constant\n",
    "c_vector = 1e-6\n",
    "\n",
    "# Setup logging\n",
    "log_dir = 'runs/simple_mf_01_' + str(datetime.now()).replace(' ', '_')\n",
    "print(log_dir)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_enc = n_item * 10 + 1\n",
    "n_item_dec = n_item + 1\n",
    "model = MFAE(n_item_enc, n_item_dec, writer=writer, k=k, c_vector=c_vector)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, model.loss)\n",
    "metrics = {'accuracy': MeanSquaredError()}\n",
    "evaluat = create_supervised_evaluator(model, metrics=metrics)\n",
    "\n",
    "train_loader = Loader(train_x, train_y, batchsize=1024, do_shuffle=False)\n",
    "test_loader = Loader(test_x, test_y, batchsize=1024, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFAE()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = []\n",
    "\n",
    "def log_training_loss(engine, log_interval=1):\n",
    "    global outs\n",
    "    outs.append(engine.state.output)\n",
    "    model.itr = engine.state.iteration\n",
    "    if model.itr % log_interval == 0:\n",
    "        fmt = \"Epoch[{}] Iteration[{}/{}] Loss: {:.4f} Avg: {:.4f}\"\n",
    "        avg = np.mean(outs)\n",
    "        msg = fmt.format(engine.state.epoch, engine.state.iteration, \n",
    "                         len(train_loader), engine.state.output, avg)\n",
    "        print(msg)\n",
    "        outs = []\n",
    "\n",
    "trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_training_loss)\n",
    "\n",
    "def log_validation_results(engine):\n",
    "    # When triggered, run the validation set\n",
    "    model.frac = 0.0\n",
    "    evaluat.run(test_loader)\n",
    "    avg_accuracy = evaluat.state.metrics['accuracy']\n",
    "    print(\"Epoch[{}] Validation MSE: {:.4f} \".format(engine.state.epoch, avg_accuracy))\n",
    "    writer.add_scalar(\"validation/avg_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "    model.frac = 0.5\n",
    "\n",
    "trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[879/879] Loss: 0.61 Avg: 0.6138\n",
      "Epoch[1] Validation MSE: 0.8787 \n",
      "Epoch[2] Iteration[1758/879] Loss: 0.61 Avg: 0.6100\n",
      "Epoch[2] Validation MSE: 0.8822 \n",
      "Epoch[3] Iteration[2637/879] Loss: 0.61 Avg: 0.6137\n",
      "Epoch[3] Validation MSE: 0.8853 \n",
      "Epoch[4] Iteration[3516/879] Loss: 0.61 Avg: 0.6092\n",
      "Epoch[4] Validation MSE: 0.8807 \n",
      "Epoch[5] Iteration[4395/879] Loss: 0.60 Avg: 0.6050\n",
      "Epoch[5] Validation MSE: 0.8751 \n",
      "Epoch[6] Iteration[5274/879] Loss: 0.60 Avg: 0.6038\n",
      "Epoch[6] Validation MSE: 0.8825 \n",
      "Epoch[7] Iteration[6153/879] Loss: 0.61 Avg: 0.6057\n",
      "Epoch[7] Validation MSE: 0.8887 \n",
      "Epoch[8] Iteration[7032/879] Loss: 0.61 Avg: 0.6123\n",
      "Epoch[8] Validation MSE: 0.8752 \n",
      "Epoch[9] Iteration[7911/879] Loss: 0.62 Avg: 0.6204\n",
      "Epoch[9] Validation MSE: 0.8738 \n",
      "Epoch[10] Iteration[8790/879] Loss: 0.60 Avg: 0.6042\n",
      "Epoch[10] Validation MSE: 0.8739 \n",
      "Epoch[11] Iteration[9669/879] Loss: 0.61 Avg: 0.6125\n",
      "Epoch[11] Validation MSE: 0.8810 \n",
      "Epoch[12] Iteration[10548/879] Loss: 0.61 Avg: 0.6107\n",
      "Epoch[12] Validation MSE: 0.8745 \n",
      "Epoch[13] Iteration[11427/879] Loss: 0.62 Avg: 0.6235\n",
      "Epoch[13] Validation MSE: 0.8794 \n",
      "Epoch[14] Iteration[12306/879] Loss: 0.61 Avg: 0.6054\n",
      "Epoch[14] Validation MSE: 0.8800 \n",
      "Epoch[15] Iteration[13185/879] Loss: 0.60 Avg: 0.6050\n",
      "Epoch[15] Validation MSE: 0.8835 \n",
      "Epoch[16] Iteration[14064/879] Loss: 0.61 Avg: 0.6112\n",
      "Epoch[16] Validation MSE: 0.8884 \n",
      "Epoch[17] Iteration[14943/879] Loss: 0.62 Avg: 0.6163\n",
      "Epoch[17] Validation MSE: 0.8784 \n",
      "Epoch[18] Iteration[15822/879] Loss: 0.61 Avg: 0.6145\n",
      "Epoch[18] Validation MSE: 0.8815 \n",
      "Epoch[19] Iteration[16701/879] Loss: 0.61 Avg: 0.6080\n",
      "Epoch[19] Validation MSE: 0.8792 \n",
      "Epoch[20] Iteration[17580/879] Loss: 0.61 Avg: 0.6129\n",
      "Epoch[20] Validation MSE: 0.8744 \n",
      "Epoch[21] Iteration[18459/879] Loss: 0.61 Avg: 0.6051\n",
      "Epoch[21] Validation MSE: 0.8757 \n",
      "Epoch[22] Iteration[19338/879] Loss: 0.61 Avg: 0.6094\n",
      "Epoch[22] Validation MSE: 0.8774 \n",
      "Epoch[23] Iteration[20217/879] Loss: 0.61 Avg: 0.6113\n",
      "Epoch[23] Validation MSE: 0.8748 \n",
      "Epoch[24] Iteration[21096/879] Loss: 0.62 Avg: 0.6199\n",
      "Epoch[24] Validation MSE: 0.8761 \n",
      "Epoch[25] Iteration[21975/879] Loss: 0.60 Avg: 0.6033\n",
      "Epoch[25] Validation MSE: 0.8782 \n",
      "Epoch[26] Iteration[22854/879] Loss: 0.60 Avg: 0.6001\n",
      "Epoch[26] Validation MSE: 0.8783 \n",
      "Epoch[27] Iteration[23733/879] Loss: 0.64 Avg: 0.6411\n",
      "Epoch[27] Validation MSE: 0.8777 \n",
      "Epoch[28] Iteration[24612/879] Loss: 0.61 Avg: 0.6130\n",
      "Epoch[28] Validation MSE: 0.8751 \n",
      "Epoch[29] Iteration[25491/879] Loss: 0.61 Avg: 0.6134\n",
      "Epoch[29] Validation MSE: 0.8876 \n",
      "Epoch[30] Iteration[26370/879] Loss: 0.61 Avg: 0.6079\n",
      "Epoch[30] Validation MSE: 0.8750 \n",
      "Epoch[31] Iteration[27249/879] Loss: 0.61 Avg: 0.6138\n",
      "Epoch[31] Validation MSE: 0.8819 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7ec8a5fb9654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/simple_mf/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspect vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "cols = ['item_id', 'title', 'tags']\n",
    "df = pd.read_csv(\"../data/ml-1m/movies.dat\", delimiter=\"::\", engine=\"python\", names=cols)\n",
    "\n",
    "label_item = [str(iid) for iid in range(df.item_id.max() + 1)]\n",
    "item_label = {}\n",
    "for item, title in zip(df.item_id, df.title):\n",
    "    label_item[item] = title\n",
    "    item_label[title] = item\n",
    "    \n",
    "label_user = [str(uid) for uid in range(n_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_label['Star Trek: Generations (1994)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = model.decoder_vect.data.numpy().copy()\n",
    "#lib = lib / np.sqrt(((lib**2.0).sum(1)[:, None] + 1e-9))\n",
    "vec = lib[329]\n",
    "sim = ((lib - vec[None, :])**2.0).sum(1)\n",
    "#sim = (lib * vec[None, :]).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Trek: Generations (1994)',\n",
       " 'Star Trek: Insurrection (1998)',\n",
       " 'Star Trek III: The Search for Spock (1984)',\n",
       " 'NeverEnding Story II: The Next Chapter, The (1990)',\n",
       " 'Amistad (1997)',\n",
       " 'Licence to Kill (1989)',\n",
       " 'Star Trek V: The Final Frontier (1989)',\n",
       " 'Star Trek: First Contact (1996)',\n",
       " 'Mole People, The (1956)',\n",
       " 'Song of the South (1946)']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[label_item[l] for l in np.argsort(sim)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
